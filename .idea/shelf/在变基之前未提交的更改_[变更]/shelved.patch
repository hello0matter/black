Index: blackgo_tool/repeat_scan_html_tool/repeat_scan_html_tool/spiders/test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import scrapy\r\n\r\nfrom repeat_scan_html_tool.items import RepeatScanHtmlToolItem\r\n\r\n\r\nclass TestSpidertype2(scrapy.Spider):\r\n    name = 'testtype2'\r\n    # allowed_domains = ['www.pzcode.cn']\r\n    num = 8707443\r\n    urltype = 'https://www.pzcode.cn/Manage/Basic/Battery/ScanView.aspx?type=2'\r\n    start_urls = [urltype + '&id=' + str(num)]\r\n\r\n    # 根据合一码扫\r\n    def parse(self, response):\r\n        inputs = response.xpath('//*[@id=\"btnDc\"]').xpath('string(.)').extract_first()\r\n        self.num = self.num + 1\r\n        items = RepeatScanHtmlToolItem()\r\n        items['input'] = inputs\r\n        items['url'] = self.urltype + '&id=' + str(self.num)\r\n        items['car'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[2]/div/div/div[13]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['hp'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[4]/div/div/div[1]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['type'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[3]/div/div/div[6]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['type2'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[2]/div/div/div[10]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['data'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[3]/div/div/div[8]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['data2'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[2]/div/div/div[15]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        first = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[3]/div/div/div[2]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        if first:\r\n            items['dc'] = first.strip()\r\n\r\n        items['qy'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[3]/div/div/div[4]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['qy2'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[2]/div/div/div[8]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        yield items\r\n        # 下载后获得的response由parse_topic处理\r\n        yield scrapy.Request(\r\n            url=self.urltype + '&id=' + str(self.num),\r\n            callback=self.parse\r\n        )\r\n\r\n\r\nclass TestSpidertype1(scrapy.Spider):\r\n    name = 'testtype1'\r\n    # allowed_domains = ['www.pzcode.cn']\r\n    num = 71273078\r\n    urltype = 'https://www.pzcode.cn/Manage/Basic/Battery/ScanView.aspx?type=1'\r\n    start_urls = [urltype + '&id=' + str(num)]\r\n\r\n    # 根据合一码扫\r\n    def parse(self, response):\r\n        inputs = response.xpath('//*[@id=\"btnDc\"]').xpath('string(.)').extract_first()\r\n        self.num = self.num + 1\r\n        items = RepeatScanHtmlToolItem()\r\n        items['input'] = inputs\r\n        items['url'] = self.urltype + '&id=' + str(self.num)\r\n        items['car'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[2]/div/div/div[13]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['hp'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[4]/div/div/div[1]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['type'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[3]/div/div/div[6]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['type2'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[2]/div/div/div[10]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['data'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[3]/div/div/div[8]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['data2'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[2]/div/div/div[15]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        first = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[3]/div/div/div[2]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        if first:\r\n            items['dc'] = first.strip()\r\n\r\n        items['qy'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[3]/div/div/div[4]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['qy2'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[2]/div/div/div[8]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        yield items\r\n        # 下载后获得的response由parse_topic处理\r\n        yield scrapy.Request(\r\n            url=self.urltype + '&id=' + str(self.num),\r\n            callback=self.parse\r\n        )\r\n\r\n\r\n# 往上都是扫车码绑定关系所有信息或者码牌号码关联绑定信息\r\nclass TestSpidertype0(scrapy.Spider):\r\n    name = 'testtype0'\r\n    # allowed_domains = ['www.pzcode.cn']\r\n    num = 8703101\r\n    urltype = 'https://www.pzcode.cn/Manage/Basic/Battery/ScanView.aspx?type=0'\r\n    start_urls = [urltype + '&id=' + str(num)]\r\n\r\n    # 根据合一码扫\r\n    def parse(self, response):\r\n        inputs = response.xpath('//*[@id=\"btnDc\"]').xpath('string(.)').extract_first()\r\n        self.num = self.num + 1\r\n        items = RepeatScanHtmlToolItem()\r\n        items['input'] = inputs\r\n        items['url'] = self.urltype + '&id=' + str(self.num)\r\n        items['car'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[2]/div/div/div[13]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['hp'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[4]/div/div/div[1]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['type'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[3]/div/div/div[6]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['type2'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[2]/div/div/div[10]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['data'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[3]/div/div/div[8]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['data2'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[2]/div/div/div[15]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        first = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[3]/div/div/div[2]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        if first:\r\n            items['dc'] = first.strip()\r\n\r\n        items['qy'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[3]/div/div/div[4]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        items['qy2'] = response.xpath('//*[@id=\"form1\"]/div[2]/div[1]/div[2]/div[2]/div/div/div[8]/span[2]').xpath(\r\n            'string(.)').extract_first()\r\n        yield items\r\n        # 下载后获得的response由parse_topic处理\r\n        yield scrapy.Request(\r\n            url=self.urltype + '&id=' + str(self.num),\r\n            callback=self.parse\r\n        )\r\n\r\n\r\n# 扫电池码中有车型的\r\nclass TestSpider(scrapy.Spider):\r\n    name = 'test'\r\n    # allowed_domains = ['www.pzcode.cn']\r\n    num = 17\r\n    urltype = 'http://pzcode.cn/pwb/MA2RJ6WHBBJCJB1DSM8012X059'\r\n    start_urls = [urltype + str(num)]\r\n\r\n    # 根据电池码扫\r\n    def parse(self, response):\r\n        # 下载后获得的response由parse_topic处理\r\n\r\n        inputs = response.xpath('//*[@id=\"btnDc\"]').xpath('string(.)').extract_first()\r\n        if inputs:\r\n            items = RepeatScanHtmlToolItem()\r\n            items['input'] = inputs\r\n\r\n            items['url'] = self.urltype + str(self.num)\r\n\r\n            items['car'] = response.xpath('/html/body/form/div[2]/div[1]/div[2]/div[2]/div/div/div[13]/span[2]').xpath(\r\n                'string(.)').extract_first()\r\n        yield items\r\n        self.num = self.num + 1\r\n\r\n        yield scrapy.Request(\r\n            url=self.urltype + str(self.num),\r\n            callback=self.parse\r\n        )\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/blackgo_tool/repeat_scan_html_tool/repeat_scan_html_tool/spiders/test.py b/blackgo_tool/repeat_scan_html_tool/repeat_scan_html_tool/spiders/test.py
--- a/blackgo_tool/repeat_scan_html_tool/repeat_scan_html_tool/spiders/test.py	(revision 02f3b8e757f0ab016b88a82830125d8b6bfae148)
+++ b/blackgo_tool/repeat_scan_html_tool/repeat_scan_html_tool/spiders/test.py	(date 1670315207502)
@@ -93,7 +93,7 @@
 class TestSpidertype0(scrapy.Spider):
     name = 'testtype0'
     # allowed_domains = ['www.pzcode.cn']
-    num = 8703101
+    num = 12627194
     urltype = 'https://www.pzcode.cn/Manage/Basic/Battery/ScanView.aspx?type=0'
     start_urls = [urltype + '&id=' + str(num)]
 
Index: blackgo_tool/repeat_scan_html_tool/repeat_scan_html_tool/spiders/mainscan.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#!/usr/bin/env python\r\n#-*- coding:utf-8 -*-\r\n\r\nfrom scrapy.cmdline import execute\r\nimport os\r\nimport sys\r\n\r\n#添加当前项目的绝对地址\r\nsys.path.append(os.path.dirname(os.path.abspath(__file__)))\r\n#执行 scrapy 内置的函数方法execute，  使用 crawl 爬取并调试，最后一个参数jobbole 是我的爬虫文件名\r\nexecute(['scrapy', 'crawl', 'testtype1'])
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/blackgo_tool/repeat_scan_html_tool/repeat_scan_html_tool/spiders/mainscan.py b/blackgo_tool/repeat_scan_html_tool/repeat_scan_html_tool/spiders/mainscan.py
--- a/blackgo_tool/repeat_scan_html_tool/repeat_scan_html_tool/spiders/mainscan.py	(revision 02f3b8e757f0ab016b88a82830125d8b6bfae148)
+++ b/blackgo_tool/repeat_scan_html_tool/repeat_scan_html_tool/spiders/mainscan.py	(date 1670315207488)
@@ -8,4 +8,4 @@
 #添加当前项目的绝对地址
 sys.path.append(os.path.dirname(os.path.abspath(__file__)))
 #执行 scrapy 内置的函数方法execute，  使用 crawl 爬取并调试，最后一个参数jobbole 是我的爬虫文件名
-execute(['scrapy', 'crawl', 'testtype1'])
\ No newline at end of file
+execute(['scrapy', 'crawl', 'testtype0'])
\ No newline at end of file
